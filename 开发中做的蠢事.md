# 2020-02-20
- 今天需要捞出一张表的数据（xxId），然后插入另一张表中，数据量比较大，200W，并且xxId会重复，需要去重之后再插入另一张表。
- 一开始我选择查询出所有的数据，存入Set中去重，此时剩余60W，之后每次10000W条插入表中，直接OOM。
- 查询的时候通过limit每次捞出1W条数据，去重插入，此时原表xxId重复，然后导致前后两次去重插入后xxId还是重复了。
- 去重通过distinct，每次捞出1w条，然后插入...  效率巨慢，相当于要进行60次distinct....